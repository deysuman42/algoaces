from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import AzureChatOpenAI
from langchain.callbacks import get_openai_callback
from langchain_core.runnables import Runnable
import streamlit as st


system_prompt = """
You are a helpful assistant that helps the user to generate reports from tables.

** IF THE DATA DOES NOT CONTAIN SUFFICIENT INFORMATION TO ANSWER THE QUESTION, THEN SAY "I DON'T KNOW" AND DO NOT MAKE ANY ASSUMPTIONS. \
    DO NOT INCLUDE ANY OTHER TEXT OR EXPLANATION. DO NOT INCLUDE THE OUTPUT FROM INDIVIDUAL PERSONAS**

"""

instruction_prompt = """ You will have two personalities:
1. You are a 'Data Analyst' that helps the user to generate insigts and reports from the inout data enclosed within **Context**. 
2. You are a 'Reviewer' who will validate the quantitative numbers generated by the data analyst and provide feedback on the report.
3. You are a 'Refiner' who will refine the report generated by the data analyst and reviewer.
4. You are a 'Finalizer' who will finalize the report and provide the final output.

** IF THE DATA DOES NOT CONTAIN SUFFICIENT INFORMATION TO ANSWER THE QUESTION, THEN SAY "I DON'T KNOW. \
    There is not sufficient information to answer this question." AND DO NOT MAKE ANY ASSUMPTIONS. \
    DO NOT INCLUDE ANY OTHER TEXT OR EXPLANATION. DO NOT INCLUDE THE OUTPUT FROM INDIVIDUAL PERSONAS**

Act as a 'Data Analyst' first and and answer the question enclosed within **Question** accurately from the data enclosed within **Context**. \
    Think step by step while calculating.

**Question**:
{question}
End of Question

**Context**:
{context}
End of Context


Next, act as a 'Reviewer' and validate the report generated by the 'Data Analyst'. Provide feedback on the report and validate the quantitaive numbers. \
    Highlight any errors or inconsistencies in the report.

Next, act as a 'Refiner' and refine the report generated by the 'Data Analyst' and 'Reviewer'. Provide a more polished and professional version of the report.

Finally, act as a 'Finalizer' and finalize the report. Provide the final output of the report. \
    Provide the final output of the report in a well-structured format.

**THE FINAL OUTPUT OF THE REPORT SHOULD BE IN A WELL-STRUCTURED FORMAT. DO NOT INCLUDE ANY OTHER TEXT OR EXPLANATION. DO NOT INCLUDE THE OUTPUT FROM INDIVIDUAL PERSONAS**

"""


def llm_openai(primary_model, api_version, temperature=0, max_tokens=None, timeout=None, max_retries=2):

    llm = AzureChatOpenAI(
    azure_deployment=primary_model,  # or your deployment
    api_version=api_version,  # or your api version
    temperature=temperature,
    max_tokens=max_tokens,
    timeout=timeout,
    max_retries=max_retries,

    )
        
    return llm


def get_prompt(system_prompt, instruction_prompt):
    """
    Returns a ChatPromptTemplate object with the system and instruction prompts.
    """
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", instruction_prompt),
        ]
    )
    return prompt


def get_response(primary_model, api_version, question, context):

    llm = llm_openai(primary_model, api_version)
    prompt = get_prompt(system_prompt, instruction_prompt)

    print("Prompt: ", prompt)

    # Create the chain by combining the prompt and the LLM
    chain = prompt | llm

    # Invoke the chain with the required inputs
    response = chain.invoke(
        {
            "question": question,
            "context": context,
        }
    )

    with get_openai_callback() as cb:
        response = chain.invoke(
            {
                "question": question,
                "context": context,
            }
        )

        
        token_metadata = {
            "prompt_tokens": cb.prompt_tokens,
            "completion_tokens": cb.completion_tokens,
            "total_tokens": cb.total_tokens,
            "total_cost (USD)": f"{cb.total_cost:.6f}",  
        }
     
    return response.content, token_metadata

def get_response_streaming(primary_model, api_version, question, context):
    """Modified function that separately handles streaming and metadata collection"""
    llm = llm_openai(primary_model, api_version)
    llm.streaming = True
    prompt = get_prompt(system_prompt, instruction_prompt)
    chain = prompt | llm
    inputs = {"question": question, "context": context}
    
    # Create and return the streaming function
    def streaming_fn():
        with get_openai_callback() as cb:
            stream = chain.stream(inputs)
            chunks = []
            
            # Process all chunks first
            for chunk in stream:
                if hasattr(chunk, "content") and chunk.content:
                    chunks.append(chunk.content)
                    yield chunk.content
            
            # Store metadata for later retrieval
            # Using a global var to store metadata - not ideal but works for this case
            st.session_state.current_token_metadata = {
                "prompt_tokens": cb.prompt_tokens,
                "completion_tokens": cb.completion_tokens,
                "total_tokens": cb.total_tokens,
                "total_cost (USD)": f"{cb.total_cost:.6f}",
            }
    
    return streaming_fn
